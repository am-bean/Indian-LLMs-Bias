{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf4bbcf",
   "metadata": {},
   "source": [
    "### The following are sample codes, which tells how the various model values were obtained. They were re-run multiple times over different parts of our evaluation dataset to reach the final results. More information on how that was done can be found in the Thesis paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a295bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embeddings\n",
    "\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "model_name = 'bert-large-cased'\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input word\n",
    "word = \"Brahmin\"\n",
    "\n",
    "# Tokenize the input word\n",
    "tokens = tokenizer.tokenize(word)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "\n",
    "# Forward pass through the model\n",
    "outputs = model(input_ids)\n",
    "\n",
    "# Extract the word embedding from the last layer\n",
    "last_layer_embedding = outputs.last_hidden_state.squeeze(0)\n",
    "\n",
    "last_layer_embedding = last_layer_embedding.detach().numpy()\n",
    "\n",
    "dic[word] = last_layer_embedding\n",
    "\n",
    "# Print the word embedding\n",
    "print(f\"Word: {word}\")\n",
    "print(f\"Embedding: {last_layer_embedding}\")\n",
    "\n",
    "#Find and store Cosine similarity\n",
    "\n",
    "for word in wordsBrahmin:\n",
    "  # Assuming you have two NumPy arrays as word embeddings with different dimensions\n",
    "    embedding1_np = dic['Dalit']\n",
    "    embedding2_np = dic[word]\n",
    "\n",
    "# Determine the maximum number of dimensions between the two embeddings\n",
    "    max_dim = max(embedding1_np.shape[1], embedding2_np.shape[1])\n",
    "\n",
    "    embedding1_np = embedding1_np.reshape(1,-1)\n",
    "    embedding2_np = embedding2_np.reshape(1,-1)\n",
    "\n",
    "# Determine the maximum number of dimensions between the two embeddings\n",
    "    max_dim = max(embedding1_np.shape[1], embedding2_np.shape[1])\n",
    "\n",
    "# Pad the embeddings to have the same number of dimensions\n",
    "    embedding1_padded = np.pad(embedding1_np, ((0, 0), (0, max_dim - embedding1_np.shape[1])), constant_values=0)\n",
    "    embedding2_padded = np.pad(embedding2_np, ((0, 0), (0, max_dim - embedding2_np.shape[1])), constant_values=0)\n",
    "\n",
    "    cosine_similarity = np.dot(embedding1_padded, embedding2_padded.T)/(np.linalg.norm(embedding1_padded) * np.linalg.norm(embedding2_padded))\n",
    "    cosineDalit[word] = cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUL Score\n",
    "\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import difflib\n",
    "import nltk\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer, BertTokenizer, BertForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')\n",
    "model = AutoModelForMaskedLM.from_pretrained('xlm-roberta-large',output_hidden_states=True, output_attentions=True)\n",
    "\n",
    "model = model.eval()\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    \n",
    "mask_id = tokenizer.mask_token_id\n",
    "log_softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "stereo_inputs = [i for i in df['Stereotypical']]\n",
    "antistereo_inputs = [i for i in df['Anti-Stereotypical']]\n",
    "\n",
    "stereo_scores = []\n",
    "antistereo_scores = []\n",
    "stereo_embes = []\n",
    "antistereo_embes = []\n",
    "\n",
    "def calculate_aul(model, sentence, log_softmax, attention):\n",
    "    '''\n",
    "    Given token ids of a sequence, return the averaged log probability of\n",
    "    unmasked sequence (AULA or AUL).\n",
    "    '''\n",
    "    tokens = tokenizer(sentence, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "    # Get the token IDs and attention mask\n",
    "    input_ids = tokens['input_ids']\n",
    "    output = model(input_ids)\n",
    "    logits = output.logits.squeeze(0)\n",
    "    log_probs = log_softmax(logits)\n",
    "    input_ids = input_ids.view(-1, 1).detach()\n",
    "    token_log_probs = log_probs.gather(1, input_ids)[1:-1]\n",
    "    if attention:\n",
    "        attentions = torch.mean(torch.cat(output.attentions, 0), 0)\n",
    "        averaged_attentions = torch.mean(attentions, 0)\n",
    "        averaged_token_attentions = torch.mean(averaged_attentions, 0)\n",
    "        token_log_probs = token_log_probs.squeeze(1) * averaged_token_attentions[1:-1]\n",
    "    sentence_log_prob = torch.mean(token_log_probs)\n",
    "    score = sentence_log_prob.item()\n",
    "\n",
    "    hidden_states = output.hidden_states[-1][:,1:-1]\n",
    "    hidden_state = torch.mean(hidden_states, 1).detach().numpy()\n",
    "\n",
    "    return score, hidden_state\n",
    "\n",
    "attention = False\n",
    "\n",
    "for i in stereo_inputs:\n",
    "    stereo_score, stereo_hidden_state = calculate_aul(model, i, log_softmax, attention)\n",
    "    stereo_scores.append(stereo_score)\n",
    "    stereo_embes.append(stereo_hidden_state)\n",
    "\n",
    "for j in antistereo_inputs:\n",
    "    antistereo_score, antistereo_hidden_state = calculate_aul(model, j, log_softmax, attention)\n",
    "    antistereo_scores.append(antistereo_score)\n",
    "    antistereo_embes.append(antistereo_hidden_state)\n",
    "\n",
    "stereo_scores = np.array(stereo_scores)\n",
    "stereo_scores = stereo_scores.reshape([-1, 1])\n",
    "antistereo_scores = np.array(antistereo_scores)\n",
    "antistereo_scores = antistereo_scores.reshape([1, -1])\n",
    "bias_scores = stereo_scores > antistereo_scores\n",
    "\n",
    "diff = []\n",
    "for i in range(len(df)):\n",
    "    dif = stereo_scores[i][0] - antistereo_scores[0][i]\n",
    "    diff.append(dif)\n",
    "\n",
    "score = len(df[df['diff']>=0])/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLL Score \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import accelerate\n",
    "import bitsandbytes\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "\n",
    "model_path=\"meta-llama/Llama-2-13b-hf\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "## Same loop was repeated for anti-stereotypical, and target words (to subtract likelihood of target words to\n",
    "## measure CONDITIONAL likelihoods)\n",
    "\n",
    "# Tokenize and convert the input sentence to a tensor\n",
    "for i in df['Stereotypical']:\n",
    "    sentence = i\n",
    "    input_ids = tokenizer.encode(sentence, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        logits = outputs.logits\n",
    "\n",
    "# Calculate the negative log likelihood for each token\n",
    "    neg_log_likelihood = torch.nn.CrossEntropyLoss(reduction='none')(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "\n",
    "# Reshape the neg_log_likelihood tensor to match the original input shape\n",
    "    neg_log_likelihood = neg_log_likelihood.view(input_ids.size())\n",
    "\n",
    "# Output the negative log likelihood for each token\n",
    "    sent = 0\n",
    "    for i in range(input_ids.size(1)):\n",
    "        token = tokenizer.decode(input_ids[0, i])\n",
    "        nll_token = -neg_log_likelihood[0, i]  # Negate the value\n",
    "        sent += nll_token\n",
    "    stereo_score.append(sent.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033a1a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT 3.5 Results\n",
    "\n",
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "\n",
    "openai.api_key = 'KEY HERE'\n",
    "\n",
    "def get_completion(prompt, model='gpt-3.5-turbo-0613'): #\"gpt-3.5-turbo-0613\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        temperature=0.7,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "lis =[]\n",
    "for j in range(len(df)):\n",
    "    print(j)\n",
    "    prompt = df['GPT_Prompt_Full'][j]\n",
    "    try:\n",
    "        response = get_completion(prompt)\n",
    "        lis.append(response)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        time.sleep(60)\n",
    "        response = get_completion(prompt)\n",
    "        lis.append(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
